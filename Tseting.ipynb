{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tldextract\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction function (same as used for training)\n",
    "def extract_url_features(url):\n",
    "    features = {}\n",
    "    \n",
    "    # Parse URL components\n",
    "    parsed = urlparse(url)\n",
    "    extracted = tldextract.extract(url)\n",
    "    \n",
    "    # Basic length features\n",
    "    features['url_length'] = len(url)\n",
    "    features['domain_length'] = len(extracted.domain)\n",
    "    features['path_length'] = len(parsed.path)\n",
    "    \n",
    "    # Domain-specific features\n",
    "    features['subdomain_length'] = len(extracted.subdomain)\n",
    "    features['tld_length'] = len(extracted.suffix) if extracted.suffix else 0\n",
    "    \n",
    "    # Character distribution\n",
    "    features['num_digits'] = sum(c.isdigit() for c in url)\n",
    "    features['num_letters'] = sum(c.isalpha() for c in url)\n",
    "    features['num_special'] = len(url) - features['num_digits'] - features['num_letters']\n",
    "    \n",
    "    # Special character counts\n",
    "    features['count_dots'] = url.count('.')\n",
    "    features['count_hyphens'] = url.count('-')\n",
    "    features['count_underscores'] = url.count('_')\n",
    "    features['count_slashes'] = url.count('/')\n",
    "    features['count_equals'] = url.count('=')\n",
    "    features['count_at'] = url.count('@')\n",
    "    features['count_and'] = url.count('&')\n",
    "    features['count_question'] = url.count('?')\n",
    "    features['count_percent'] = url.count('%')\n",
    "    features['count_plus'] = url.count('+')\n",
    "    features['count_asterisk'] = url.count('*')\n",
    "    features['count_exclamation'] = url.count('!')\n",
    "    \n",
    "    # Binary features\n",
    "    features['has_ip_address'] = 1 if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url) else 0\n",
    "    features['has_http'] = 1 if 'http://' in url else 0\n",
    "    features['has_https'] = 1 if 'https://' in url else 0\n",
    "    features['has_port'] = 1 if re.search(r':\\d+', url) else 0\n",
    "    # features['has_suspicious_words'] = 1 if re.search(r'(login|bank|account|secure|update|confirm)', url.lower()) else 0\n",
    "    \n",
    "    # # Ratios and derived features\n",
    "    # features['digits_to_letters_ratio'] = features['num_digits'] / features['num_letters'] if features['num_letters'] > 0 else 0\n",
    "    # features['special_to_total_ratio'] = features['num_special'] / len(url) if len(url) > 0 else 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained XGBoost model\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        model = pickle.load(open(model_path, 'rb'))\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_scaler(scaler_path):\n",
    "    \"\"\"Load the fitted StandardScaler\"\"\"\n",
    "    if not os.path.exists(scaler_path):\n",
    "        raise FileNotFoundError(f\"Scaler file not found at {scaler_path}\")\n",
    "    \n",
    "    try:\n",
    "        scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "        return scaler\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading scaler: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_urls(urls, model, scaler=None):\n",
    "    \"\"\"\n",
    "    Predict multiple URLs using the trained model\n",
    "    \n",
    "    Args:\n",
    "        urls (list): List of URLs to predict\n",
    "        model: Trained XGBoost model\n",
    "        scaler: Fitted StandardScaler (optional)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with URLs and predictions\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for url in urls:\n",
    "        # Extract features\n",
    "        features = extract_url_features(url)\n",
    "        features_df = pd.DataFrame([features])\n",
    "        \n",
    "        # Scale features if scaler is provided\n",
    "        if scaler:\n",
    "            features_scaled = scaler.transform(features_df)\n",
    "            # Make prediction\n",
    "            prediction = model.predict(features_scaled)[0]\n",
    "            probability = model.predict_proba(features_scaled)[0]\n",
    "        else:\n",
    "            # Make prediction without scaling\n",
    "            prediction = model.predict(features_df)[0]\n",
    "            probability = model.predict_proba(features_df)[0]\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'url': url,\n",
    "            'prediction': int(prediction),\n",
    "            'label': 'Legitimate' if prediction == 1 else 'Malicious',\n",
    "            'confidence': probability[1] if prediction == 1 else probability[0]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing script\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to saved model and scaler\n",
    "    MODEL_PATH = 'xgboost_url_classifier.pkl'  # Update with your model path\n",
    "    SCALER_PATH = 'url_scaler.pkl'  # Update with your scaler path\n",
    "    \n",
    "    # Check if model exists, if not, ask user\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        MODEL_PATH = input(\"Enter the path to your XGBoost model file (.pkl): \")\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(MODEL_PATH)\n",
    "    if model is None:\n",
    "        print(\"Failed to load model. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Try to load scaler if it exists\n",
    "    scaler = None\n",
    "    if os.path.exists(SCALER_PATH):\n",
    "        scaler = load_scaler(SCALER_PATH)\n",
    "        print(\"Scaler loaded successfully.\")\n",
    "    else:\n",
    "        print(\"No scaler found. Proceeding without scaling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL Classifier Testing Tool\n",
      "===========================\n",
      "1. Test individual URLs\n",
      "2. Test URLs from a file\n",
      "File not found: \n",
      "Error reading file: [Errno 2] No such file or directory: ''\n",
      "Loaded 7 URLs from file.\n",
      "\n",
      "Prediction Summary:\n",
      "Total URLs: 7\n",
      "Legitimate URLs: 0 (0.00%)\n",
      "Malicious URLs: 7 (100.00%)\n",
      "\n",
      "Detailed results saved to url_prediction_results_20250321_102338.csv\n",
      "\n",
      "Sample predictions:\n",
      "                                                url  prediction      label  \\\n",
      "0                             https://www.apple.com           0  Malicious   \n",
      "1                            https://www.amazon.com           0  Malicious   \n",
      "2                          https://www.facebook.com           0  Malicious   \n",
      "3                   http://amazon-security-check.ga           0  Malicious   \n",
      "4  http://facebook.com-user-login-authenticate.info           0  Malicious   \n",
      "5         https://secure.google.accounts-signin.xyz           0  Malicious   \n",
      "6                                               doe           0  Malicious   \n",
      "\n",
      "   confidence  \n",
      "0    0.983415  \n",
      "1    0.983608  \n",
      "2    0.983082  \n",
      "3    0.973183  \n",
      "4    0.978072  \n",
      "5    0.999227  \n",
      "6    0.627191  \n"
     ]
    }
   ],
   "source": [
    "# Test modes\n",
    "print(\"\\nURL Classifier Testing Tool\")\n",
    "print(\"===========================\")\n",
    "print(\"1. Test individual URLs\")\n",
    "print(\"2. Test URLs from a file\")\n",
    "choice = input(\"Select option (1/2): \")\n",
    "\n",
    "if choice == '1':\n",
    "    # Individual URL testing\n",
    "    urls_to_test = []\n",
    "    while True:\n",
    "        url = input(\"\\nEnter a URL to test (or 'done' to finish): \")\n",
    "        if url.lower() == 'done':\n",
    "            break\n",
    "        urls_to_test.append(url)\n",
    "    \n",
    "    if not urls_to_test:\n",
    "        print(\"No URLs provided. Exiting.\")\n",
    "        exit(0)\n",
    "        \n",
    "    # Make predictions\n",
    "    results = predict_urls(urls_to_test, model, scaler)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"------------------\")\n",
    "    for i, row in results.iterrows():\n",
    "        print(f\"URL: {row['url']}\")\n",
    "        print(f\"Prediction: {row['label']} (Class {row['prediction']})\")\n",
    "        print(f\"Confidence: {row['confidence']:.4f}\")\n",
    "        print(\"------------------\")\n",
    "        \n",
    "elif choice == '2':\n",
    "    # File-based testing\n",
    "    file_path = input(\"Enter path to file containing URLs (one URL per line): \")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        exit(1)\n",
    "        \n",
    "    # Read URLs from file\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            urls_to_test = [line.strip() for line in f if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        exit(1)\n",
    "        \n",
    "    if not urls_to_test:\n",
    "        print(\"No URLs found in file. Exiting.\")\n",
    "        exit(0)\n",
    "        \n",
    "    print(f\"Loaded {len(urls_to_test)} URLs from file.\")\n",
    "    \n",
    "    # Make predictions\n",
    "    results = predict_urls(urls_to_test, model, scaler)\n",
    "    \n",
    "    # Display summary\n",
    "    legitimate_count = sum(results['prediction'] == 1)\n",
    "    malicious_count = sum(results['prediction'] == 0)\n",
    "    \n",
    "    print(\"\\nPrediction Summary:\")\n",
    "    print(f\"Total URLs: {len(results)}\")\n",
    "    print(f\"Legitimate URLs: {legitimate_count} ({legitimate_count/len(results)*100:.2f}%)\")\n",
    "    print(f\"Malicious URLs: {malicious_count} ({malicious_count/len(results)*100:.2f}%)\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    output_file = f\"url_prediction_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    results.to_csv(output_file, index=False)\n",
    "    print(f\"\\nDetailed results saved to {output_file}\")\n",
    "    \n",
    "    # Show a few examples\n",
    "    print(\"\\nSample predictions:\")\n",
    "    print(results.head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"Invalid choice. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              feature_weights=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=7,\n",
      "              max_leaves=None, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
      "              n_jobs=None, num_parallel_tree=None, ...)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
